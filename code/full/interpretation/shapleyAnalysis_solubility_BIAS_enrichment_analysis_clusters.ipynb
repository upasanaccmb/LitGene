{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Define Model**\n",
    "\n",
    "### Instructions\n",
    "\n",
    "**1.** Put model ```.py``` file in this directory\n",
    "\n",
    "\n",
    "**2.** If you want to use code as is and simply change ```input_data_path```,\n",
    "\n",
    "\n",
    "Put data as csv in the following format (as seen in ```/mnt/data/GeneLLM/data/solubility.csv```):\n",
    "\n",
    "\n",
    "    Gene name, StrLabel, Label, Summary\n",
    "\n",
    "    LIME1, Membrane, 0, \"This genomic region...\" \n",
    "    \n",
    "    TMEM219, Membrane, 0, \"Ceramidases (EC 3.5.1.23) ...\" \n",
    "\n",
    "\n",
    "Otherwise, somehow load a list of ```sentences``` strings, a list of ```labels``` integers, and get the number of labels ```n_labels```\n",
    "\n",
    "\n",
    "**3.** Check model hyperparameters\n",
    "\n",
    "\n",
    "**4.** Run the next block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/dandreas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from shapleyAnalysis import *\n",
    "import sys\n",
    "import os\n",
    "\n",
    "## IMPORT MODEL AND TOKENIZER ##\n",
    "from BERT import * \n",
    "from transformers import BertTokenizerFast\n",
    "state_dict_path = 'best_model_state_dict.pth'\n",
    "\n",
    "## DATA ##\n",
    "input_data_path = \"clean_genes.csv\"\n",
    "task_type = \"classification\"\n",
    "gene_loaded_data = pd.read_csv(input_data_path)\n",
    "n_labels = 2\n",
    "sentences = gene_loaded_data[\"Summary\"].tolist()\n",
    "geneNames = gene_loaded_data[\"Gene name\"].tolist()\n",
    "gene_to_idx = {gene:idx for idx, gene in enumerate(geneNames)}\n",
    "\n",
    "## HYPERPARAMETERS ## \n",
    "pool = \"mean\"\n",
    "drop_rate =0.1\n",
    "model_name = 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext'\n",
    "gene2vec_flag = False\n",
    "gene2vec_hidden = 200\n",
    "device = \"cuda\"\n",
    "tokenizer_max_length = 512\n",
    "\n",
    "## INITIALIZE MODEL AND TOKENIZER ##\n",
    "model = FineTunedBERT(pool= pool, \n",
    "                      task_type = task_type, \n",
    "                      n_labels = n_labels,\n",
    "                      drop_rate = drop_rate, \n",
    "                      model_name = model_name,\n",
    "                      gene2vec_flag= gene2vec_flag,\n",
    "                      gene2vec_hidden = gene2vec_hidden).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(state_dict_path))\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model.model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Shapley Analysis**\n",
    "\n",
    "### Instructions\n",
    "\n",
    "The analysis will save two files to the current directory.\n",
    "\n",
    "Change the ```dataset_name``` to match your dataset. This will be the prefix of the files saved by the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "12955\n"
     ]
    }
   ],
   "source": [
    "#loading\n",
    "data_dir = 'enrichment_analysis/'\n",
    "data_file_root_name = 'GeneLLM_all_cluster'\n",
    "\n",
    "#saving\n",
    "save_dir = '/home/dandreas/GeneLLM2/data/BIAS_enrichment_analysis_GeneLLM_clusters/'\n",
    "\n",
    "all_idxs=[]\n",
    "for i in range(26):\n",
    "    print(i)\n",
    "    file_path = data_dir+data_file_root_name+str(i)+'.txt'\n",
    "    with open(file_path,'r') as clusterfile:\n",
    "        cluster = [line.strip() for line in clusterfile.readlines()]\n",
    "    idxs = [gene_to_idx[g] for g in cluster]\n",
    "    all_idxs+=idxs\n",
    "    clusterSentences = [sentences[idx] for idx in idxs]\n",
    "    dataset_name='cluster'+str(i)\n",
    "    # token_analysis, word_analysis = getShapleyAnalysis_classification(clusterSentences, model, tokenizer, tokenizer_max_length, device, dataset_name, save_dir)\n",
    "print(len(all_idxs))\n",
    "print(len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### AGGREGATE SHAP VALUES ###\n",
    "def concat(x,y):\n",
    "    z=shap.Explanation(values=[])\n",
    "    z.values = np.concatenate((x.values,y.values),axis=0)\n",
    "    z.base_values = np.concatenate((x.base_values,y.base_values),axis=0)\n",
    "    z.data = x.data+y.data\n",
    "    return z\n",
    "\n",
    "shap_values_all= None\n",
    "for i in range(26):\n",
    "    file_path = save_dir + 'cluster' + str(i) + '_shap_values.pkl'\n",
    "    shap_values = pickleLoad(file_path)\n",
    "    if shap_values_all is None: shap_values_all = shap.Explanation(values = shap_values.values, base_values=shap_values.base_values, data=shap_values.data)\n",
    "    else:  \n",
    "        shap_values_all = concat(shap_values_all, shap_values)\n",
    "\n",
    "pickleSave(shap_values_all,save_dir,'all_shap_values.pkl')\n",
    "\n",
    "# dataset_name = 'all'\n",
    "# token_analysis, token_analysis_indexes = getShapValuesDictsAndIndexes(shap_values_all)\n",
    "# pickleSave(token_analysis,save_dir,dataset_name+'_token_analysis.pkl')\n",
    "# pickleSave(token_analysis_indexes,save_dir,dataset_name+'_token_analysis_indexes.pkl')\n",
    "\n",
    "# shap_values_grouped_by_word_all = getShapValuesGroupedByWord(shap_values_all)\n",
    "# word_analysis, word_analysis_indexes = getShapValuesDictsAndIndexes_groupedByWordSum(shap_values_grouped_by_word_all)\n",
    "# pickleSave(word_analysis,save_dir,dataset_name+'_word_analysis.pkl')\n",
    "# pickleSave(word_analysis_indexes,save_dir,dataset_name+'_word_analysis_indexes.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Plot**\n",
    "\n",
    "### Instructions\n",
    "\n",
    "Pass the analysis you want to plot (i.e., either ```token_anlysis``` or ```word_analysis``` above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default values, uncomment to change and pass as keyword argument to generatePlots()\n",
    "# nToPlot = 100 \n",
    "# percentile=90 \n",
    "# minOccurances = 10 \n",
    "# collapsePlural = True \n",
    "# minStringLength = 1 \n",
    "# stops = nltkStopwords \n",
    "# saveName=None # change savename to a string in order to save plots\n",
    "\n",
    "generatePlots(word_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepV_a100",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
