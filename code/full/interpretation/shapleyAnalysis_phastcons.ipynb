{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Define Model**\n",
    "\n",
    "### Instructions\n",
    "\n",
    "**1.** Put model ```.py``` file in this directory\n",
    "\n",
    "\n",
    "**2.** Change ```state_dict_path``` and ```input_data_path``` as needed\n",
    "\n",
    "\n",
    "**3.** Check model hyperparameters\n",
    "\n",
    "\n",
    "**4.** Run the next block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapleyAnalysis import *\n",
    "import sys\n",
    "import os\n",
    "\n",
    "## IMPORT MODEL AND TOKENIZER ##\n",
    "from GeneLLM import * \n",
    "from transformers import BertTokenizerFast\n",
    "state_dict_name = 'phastcons_2'\n",
    "state_dict_path = 'best_models/best_model_'+state_dict_name+'.pth'\n",
    "\n",
    "## DATA ##\n",
    "input_data_path = \"clean_genes.csv\"\n",
    "gene_loaded_data = pd.read_csv(input_data_path)\n",
    "sentences = gene_loaded_data[\"Summary\"].tolist()\n",
    "geneNames = gene_loaded_data[\"Gene name\"].tolist()\n",
    "gene_to_idx = {gene:idx for idx, gene in enumerate(geneNames)}\n",
    "\n",
    "## HYPERPARAMETERS ## \n",
    "pool= \"cls\"\n",
    "drop_rate= 0.1\n",
    "model_name= 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext'\n",
    "gene2vec_flag= False\n",
    "gene2vec_hidden= 200\n",
    "device=\"cuda:0\"\n",
    "tokenizer_max_length= 512\n",
    "n_labels= 1\n",
    "task_type='regression'\n",
    "\n",
    "## INITIALIZE MODEL AND TOKENIZER ##\n",
    "\n",
    "model = FineTunedBERT(pool= pool, \n",
    "                    task_type = task_type, \n",
    "                    n_labels = n_labels,\n",
    "                    drop_rate =  drop_rate, \n",
    "                    model_name = model_name,\n",
    "                    gene2vec_flag= gene2vec_flag,\n",
    "                    gene2vec_hidden = gene2vec_hidden).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(state_dict_path))\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model.model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Shapley Analysis**\n",
    "\n",
    "### Instructions\n",
    "Check the following parameters and run the next block.\n",
    "\n",
    "\n",
    "```save_dir``` is the directory where the SHAP values will be saved.\n",
    "\n",
    "\n",
    "```dataset_name``` will be the prefix of the shap values file saved by the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/home/dandreas/GeneLLM2/data/SHAPValues/'\n",
    "dataset_name = 'clean_genes_'+state_dict_name\n",
    "shap_values = getSHAPValues(sentences, model, tokenizer, tokenizer_max_length, device, dataset_name, save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepV_a100",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
